{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "f927553d-8906-4c57-b64c-a8d195c48b7e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark import SparkContext, SparkConf\n",
    "import pyspark\n",
    "import pandas as pd\n",
    "from pyspark.sql.utils import AnalysisException\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "7766664c-5f82-439f-8864-b0eee9cae2b0",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Setting spark.hadoop.yarn.resourcemanager.principal to hvega.externo\n"
     ]
    }
   ],
   "source": [
    "ss_name = 'Ejecucion algoritmo IVA Credito Remanente'\n",
    "wg_conn = \"spark.kerberos.access.hadoopFileSystems\"\n",
    "db_conn = \"abfs://data@datalakesii.dfs.core.windows.net/\"\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "      .appName(f\"Ejecucion algoritmo {ss_name}\")  \\\n",
    "      .config(wg_conn, db_conn) \\\n",
    "      .config(\"spark.executor.memory\", \"6g\") \\\n",
    "      .config(\"spark.driver.memory\", \"12g\")\\\n",
    "      .config(\"spark.executor.cores\", \"4\") \\\n",
    "      .config(\"spark.executor.instances\", \"5\") \\\n",
    "      .config(\"spark.driver.maxResultSize\", \"12g\") \\\n",
    "      .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "spark.conf.set(\"spark.sql.parquet.enableVectorizedReader\",\"false\")\n",
    "spark.conf.set(\"spark.sql.parquet.int96RebaseModeInRead\", \"CORRECTED\")\n",
    "spark.conf.set(\"spark.sql.parquet.int96RebaseModeInWrite\", \"CORRECTED\")\n",
    "spark.conf.set(\"spark.sql.parquet.datetimeRebaseModeInRead\", \"CORRECTED\")\n",
    "spark.conf.set(\"spark.sql.parquet.datetimeRebaseModeInWrite\", \"CORRECTED\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dc443548-b2ef-4f43-9246-71a63af644ee",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "df_spark=spark.read.parquet(\"abfs://data@datalakesii.dfs.core.windows.net/DatosOrigen/lr-629/riesgo_fraude/remanente_contaminado/remanente_hist_updated\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "b3d1de9d-2098-4c9e-beba-d068ef8e16ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_spark=spark.sql('select * from dw.dw_trn_f29_e')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6f01085c-8665-48bd-b929-4b2bf19513d2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de columnas: 365\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 0:======================================================>(202 + 1) / 203]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Número de filas: 113122399\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "# Número de columnas\n",
    "num_columns = len(df_spark.columns)\n",
    "print(f\"Número de columnas: {num_columns}\")\n",
    "\n",
    "# Número de filas\n",
    "num_rows = df_spark.count()\n",
    "print(f\"Número de filas: {num_rows}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32bac793-a6e2-4041-9c44-e8fa85bd0b43",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Seleccionar los valores únicos de la columna \"columna_ejemplo\"\n",
    "valores_unicos_ordenados = df_spark.select(\"dcv_ptributario\").distinct().orderBy(col(\"dcv_ptributario\").desc())\n",
    "\n",
    "\n",
    "# Mostrar los valores únicos\n",
    "valores_unicos_ordenados.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b011b585-b80a-4ef6-8b42-c2ff54656e93",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "520e7220-97c1-4e03-b01f-26cf7b5a0c9f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pyspark.sql.functions as F\n",
    "\n",
    "# Convertir columnas de fecha y hora a cadenas en un solo paso\n",
    "def convert_date_time_columns(df):\n",
    "    date_time_cols = [c for c in df.columns if 'timestamp' in df.schema[c].dataType.simpleString() or 'date' in df.schema[c].dataType.simpleString()]\n",
    "    for col_name in date_time_cols:\n",
    "        df = df.withColumn(col_name, F.date_format(F.col(col_name), \"yyyy-MM-dd HH:mm:ss\"))\n",
    "    return df\n",
    "\n",
    "# Aplicar la conversión\n",
    "df_spark_converted = convert_date_time_columns(df_spark)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b38a6ed3-8c3e-4fd1-9ba4-3ba5ec0fd3ab",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Guardar el DataFrame de Spark como un archivo CSV\n",
    "df_spark_converted.write \\\n",
    "    .format(\"csv\") \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .mode(\"overwrite\") \\\n",
    "    .save(\"/home/cdsw/data/remanente_ejemplo.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ae7370c-3780-44f2-9120-663a2bea4c30",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d1e0655-fd89-45ab-a883-b4bbd90aeca0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "def generate_periods_from_today_as_integers(num_years):\n",
    "    # Obtener la fecha actual\n",
    "    today = datetime.today()\n",
    "    \n",
    "    # Inicializar la lista de periodos\n",
    "    periods = []\n",
    "    \n",
    "    # Fecha de inicio es el primer día del mes actual\n",
    "    start_date = today.replace(day=1)\n",
    "    \n",
    "    # Calcular la fecha final (hace num_years años)\n",
    "    end_date = start_date - timedelta(days=num_years * 365)\n",
    "    \n",
    "    # Generar periodos en formato YYYYMM\n",
    "    current_date = start_date\n",
    "    while current_date > end_date:\n",
    "        # Convertir el periodo a entero y añadir a la lista al inicio\n",
    "        period_as_int = int(current_date.strftime(\"%Y%m\"))\n",
    "        periods.insert(0, period_as_int)  # Inserta al principio de la lista\n",
    "        # Mover al mes anterior\n",
    "        next_month = current_date.month - 1 if current_date.month > 1 else 12\n",
    "        next_year = current_date.year - 1 if current_date.month == 1 else current_date.year\n",
    "        current_date = current_date.replace(year=next_year, month=next_month, day=1)\n",
    "    \n",
    "    # Omitir el último periodo (mes en el cual me detengo)\n",
    "    if periods:\n",
    "        periods.pop(0)\n",
    "    \n",
    "    return periods\n",
    "# Ejemplo de uso\n",
    "num_years = 3\n",
    "periods_list = generate_periods_from_today_as_integers(num_years)\n",
    "print(periods_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad507e52-b396-4cdd-9921-84617cc9d1ad",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "def generate_periods_from_today_as_integers(num_years):\n",
    "    # Obtener la fecha actual\n",
    "    today = datetime.today()\n",
    "    \n",
    "    # Inicializar la lista de periodos\n",
    "    periods = []\n",
    "    \n",
    "    # Fecha de inicio es el primer día del mes actual\n",
    "    start_date = today.replace(day=1)\n",
    "    \n",
    "    # Calcular la fecha final (hace num_years años), no incluyendo el mes actual\n",
    "    end_date = start_date - timedelta(days=num_years * 365)\n",
    "    \n",
    "    # Ajustar end_date para que sea el primer día del mes del periodo que queremos excluir\n",
    "    end_date = end_date.replace(day=1)\n",
    "    \n",
    "    # Generar periodos en formato YYYYMM\n",
    "    current_date = start_date\n",
    "    while current_date > end_date:\n",
    "        # Convertir el periodo a entero y añadir a la lista al inicio\n",
    "        period_as_int = int(current_date.strftime(\"%Y%m\"))\n",
    "        periods.insert(0, period_as_int)  # Inserta al principio de la lista\n",
    "        # Mover al mes anterior\n",
    "        next_month = current_date.month - 1 if current_date.month > 1 else 12\n",
    "        next_year = current_date.year - 1 if current_date.month == 1 else current_date.year\n",
    "        current_date = current_date.replace(year=next_year, month=next_month, day=1)\n",
    "    \n",
    "    # El último periodo será el mes anterior al actual\n",
    "    if periods:\n",
    "        periods.pop(0)\n",
    "    \n",
    "    return periods\n",
    "\n",
    "# Ejemplo de uso\n",
    "num_years = 3\n",
    "periods_list = generate_periods_from_today_as_integers(num_years)\n",
    "print(periods_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8bd74ad2-f669-4516-8420-49c98525d887",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datetime import datetime, timedelta\n",
    "\n",
    "def generate_periods_from_today_as_integers(num_years):\n",
    "    # Obtener la fecha actual\n",
    "    today = datetime.today()\n",
    "    \n",
    "    # Inicializar la lista de periodos\n",
    "    periods = []\n",
    "    \n",
    "    # Fecha de inicio es el primer día del mes actual\n",
    "    start_date = today.replace(day=1)\n",
    "    \n",
    "    # Calcular la fecha final (hace num_years años)\n",
    "    end_date = start_date - timedelta(days=num_years * 365)\n",
    "    \n",
    "    # Ajustar end_date para que sea el primer día del mes del periodo que queremos excluir\n",
    "    end_date = end_date.replace(day=1)\n",
    "    \n",
    "    # Generar periodos en formato YYYYMM\n",
    "    current_date = start_date\n",
    "    while current_date > end_date:\n",
    "        # Convertir el periodo a entero y añadir a la lista al inicio\n",
    "        period_as_int = int(current_date.strftime(\"%Y%m\"))\n",
    "        periods.insert(0, period_as_int)  # Inserta al principio de la lista\n",
    "        # Mover al mes anterior\n",
    "        next_month = current_date.month - 1 if current_date.month > 1 else 12\n",
    "        next_year = current_date.year - 1 if current_date.month == 1 else current_date.year\n",
    "        current_date = current_date.replace(year=next_year, month=next_month, day=1)\n",
    "    \n",
    "    # Eliminar el último valor de la lista si no está vacío\n",
    "    if periods:\n",
    "        periods.pop()  # Elimina el último elemento\n",
    "    \n",
    "    return periods\n",
    "\n",
    "# Ejemplo de uso\n",
    "num_years = 3\n",
    "periods_list = generate_periods_from_today_as_integers(num_years)\n",
    "print(periods_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "eba71be1-9b37-45fa-a117-b4a8529ca9c4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "StructType(List(StructField(max_folio,StringType,true),StructField(cont_rut,StringType,true),StructField(periodo,IntegerType,true)))"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "a=spark.read.parquet(\"abfs://data@datalakesii.dfs.core.windows.net/DatosOrigen/lr-629/riesgo_fraude/remanente_contaminado/f29_unico\")\n",
    "a.registerTempTable('f29')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "35a859e4-fa1c-41b2-b2d7-d349caa4e80f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Hive Session ID = f05b230c-ded8-407d-9572-d5d6b6aff69e\n",
      "[Stage 8:>                                                          (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------------+\n",
      "|f29_agno_mes_tributario_vo|\n",
      "+--------------------------+\n",
      "|                    202307|\n",
      "|                    202306|\n",
      "|                    202305|\n",
      "|                    202304|\n",
      "|                    202303|\n",
      "|                    202302|\n",
      "|                    202301|\n",
      "|                    202212|\n",
      "|                    202211|\n",
      "|                    202210|\n",
      "|                    202209|\n",
      "|                    202208|\n",
      "|                    202207|\n",
      "|                    202206|\n",
      "|                    202205|\n",
      "|                    202204|\n",
      "|                    202203|\n",
      "|                    202202|\n",
      "|                    202201|\n",
      "|                    202112|\n",
      "+--------------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql('select distinct(f29_agno_mes_tributario_vo) from dw.dw_trn_f29_e order by f29_agno_mes_tributario_vo desc').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d81e945b-7b38-4c0d-82d1-ba7db445a156",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[Stage 11:>                                                         (0 + 1) / 1]\r"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+---------------+\n",
      "|dcv_ptributario|\n",
      "+---------------+\n",
      "|         202402|\n",
      "|         202401|\n",
      "|         202312|\n",
      "|         202311|\n",
      "|         202310|\n",
      "|         202309|\n",
      "|         202308|\n",
      "|         202307|\n",
      "|         202306|\n",
      "|         202305|\n",
      "|         202304|\n",
      "|         202303|\n",
      "|         202302|\n",
      "|         202301|\n",
      "|         202212|\n",
      "|         202211|\n",
      "|         202210|\n",
      "|         202209|\n",
      "|         202208|\n",
      "|         202207|\n",
      "+---------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    }
   ],
   "source": [
    "spark.sql('select distinct(dcv_ptributario) from dwbgdata.dcv_generic_det_consolidado_sas order by dcv_ptributario desc').show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1192920-91ab-4b73-a79b-c24e89eae6eb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
