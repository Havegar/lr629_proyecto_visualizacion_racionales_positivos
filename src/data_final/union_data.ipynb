{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e8992d4c-fd01-4309-bd76-6e4a4d54b949",
   "metadata": {},
   "source": [
    "### Union de data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "e13af7a7-8411-4a40-b2ca-f432963bca07",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark import SparkContext, SparkConf\n",
    "import pyspark\n",
    "import pandas as pd\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=DeprecationWarning)\n",
    "from pyspark.sql.types import StringType,TimestampType\n",
    "import matplotlib.pyplot as plt\n",
    "from pyspark.sql import functions as F\n",
    "from pyspark.sql.window import Window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c94cfd21-1ccf-4082-ad45-572ade5533f9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "ss_name = 'Agregación de labels a contaminados'\n",
    "wg_conn = \"spark.kerberos.access.hadoopFileSystems\"\n",
    "db_conn = \"abfs://data@datalakesii.dfs.core.windows.net/\"\n",
    "\n",
    "spark = SparkSession.builder \\\n",
    "      .appName(f\"Ejecucion algoritmo {ss_name}\")  \\\n",
    "      .config(wg_conn, db_conn) \\\n",
    "      .config(\"spark.executor.memory\", \"6g\") \\\n",
    "      .config(\"spark.driver.memory\", \"12g\")\\\n",
    "      .config(\"spark.executor.cores\", \"4\") \\\n",
    "      .config(\"spark.executor.instances\", \"10\") \\\n",
    "      .config(\"spark.driver.maxResultSize\", \"12g\") \\\n",
    "      .getOrCreate()\n",
    "\n",
    "spark.sparkContext.setLogLevel(\"ERROR\")\n",
    "\n",
    "spark.conf.set(\"spark.sql.parquet.enableVectorizedReader\",\"false\")\n",
    "spark.conf.set(\"spark.sql.parquet.int96RebaseModeInRead\", \"CORRECTED\")\n",
    "spark.conf.set(\"spark.sql.parquet.int96RebaseModeInWrite\", \"CORRECTED\")\n",
    "spark.conf.set(\"spark.sql.parquet.datetimeRebaseModeInRead\", \"CORRECTED\")\n",
    "spark.conf.set(\"spark.sql.parquet.datetimeRebaseModeInWrite\", \"CORRECTED\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "a40fe489-ce40-4e37-b4a0-f583bfc9a219",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "                                                                                \r"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[36], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m louvain\u001b[38;5;241m=\u001b[39m \u001b[43mspark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43m/home/cdsw/data/lovain_agrupacion/louvain_all.csv\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mheader\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minferSchema\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m louvain\u001b[38;5;241m.\u001b[39mcolumns\n",
      "File \u001b[0;32m/opt/spark/python/lib/pyspark.zip/pyspark/sql/readwriter.py:410\u001b[0m, in \u001b[0;36mDataFrameReader.csv\u001b[0;34m(self, path, schema, sep, encoding, quote, escape, comment, header, inferSchema, ignoreLeadingWhiteSpace, ignoreTrailingWhiteSpace, nullValue, nanValue, positiveInf, negativeInf, dateFormat, timestampFormat, maxColumns, maxCharsPerColumn, maxMalformedLogPerPartition, mode, columnNameOfCorruptRecord, multiLine, charToEscapeQuoteEscaping, samplingRatio, enforceSchema, emptyValue, locale, lineSep, pathGlobFilter, recursiveFileLookup, modifiedBefore, modifiedAfter, unescapedQuoteHandling)\u001b[0m\n\u001b[1;32m    408\u001b[0m     path \u001b[38;5;241m=\u001b[39m [path]\n\u001b[1;32m    409\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mtype\u001b[39m(path) \u001b[38;5;241m==\u001b[39m \u001b[38;5;28mlist\u001b[39m:\n\u001b[0;32m--> 410\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_df(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jreader\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcsv\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_spark\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sc\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jvm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mPythonUtils\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoSeq\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpath\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[1;32m    411\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path, RDD):\n\u001b[1;32m    412\u001b[0m     \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mfunc\u001b[39m(iterator):\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/py4j/java_gateway.py:1303\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[0;34m(self, *args)\u001b[0m\n\u001b[1;32m   1296\u001b[0m args_command, temp_args \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_build_args(\u001b[38;5;241m*\u001b[39margs)\n\u001b[1;32m   1298\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1299\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1300\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[1;32m   1301\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m-> 1303\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1304\u001b[0m return_value \u001b[38;5;241m=\u001b[39m get_return_value(\n\u001b[1;32m   1305\u001b[0m     answer, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtarget_id, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mname)\n\u001b[1;32m   1307\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/py4j/java_gateway.py:1033\u001b[0m, in \u001b[0;36mGatewayClient.send_command\u001b[0;34m(self, command, retry, binary)\u001b[0m\n\u001b[1;32m   1031\u001b[0m connection \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_get_connection()\n\u001b[1;32m   1032\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m-> 1033\u001b[0m     response \u001b[38;5;241m=\u001b[39m \u001b[43mconnection\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msend_command\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcommand\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1034\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m binary:\n\u001b[1;32m   1035\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m response, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_connection_guard(connection)\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/site-packages/py4j/clientserver.py:475\u001b[0m, in \u001b[0;36mClientServerConnection.send_command\u001b[0;34m(self, command)\u001b[0m\n\u001b[1;32m    473\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m    474\u001b[0m     \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[0;32m--> 475\u001b[0m         answer \u001b[38;5;241m=\u001b[39m smart_decode(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mreadline\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m])\n\u001b[1;32m    476\u001b[0m         logger\u001b[38;5;241m.\u001b[39mdebug(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAnswer received: \u001b[39m\u001b[38;5;132;01m{0}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;241m.\u001b[39mformat(answer))\n\u001b[1;32m    477\u001b[0m         \u001b[38;5;66;03m# Happens when a the other end is dead. There might be an empty\u001b[39;00m\n\u001b[1;32m    478\u001b[0m         \u001b[38;5;66;03m# answer before the socket raises an error.\u001b[39;00m\n",
      "File \u001b[0;32m/usr/local/lib/python3.10/socket.py:705\u001b[0m, in \u001b[0;36mSocketIO.readinto\u001b[0;34m(self, b)\u001b[0m\n\u001b[1;32m    703\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;28;01mTrue\u001b[39;00m:\n\u001b[1;32m    704\u001b[0m     \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m--> 705\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sock\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrecv_into\u001b[49m\u001b[43m(\u001b[49m\u001b[43mb\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    706\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m timeout:\n\u001b[1;32m    707\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_timeout_occurred \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "louvain= spark.read.csv(\"/home/cdsw/data/lovain_agrupacion/louvain_all.csv\", header=True, inferSchema=True)\n",
    "louvain.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07c0b3e0-4703-4de8-9e7a-5f2b49132989",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "louvain.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "207bd4c9-5b0f-4118-ad2a-062bad596921",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "contaminacion = spark.read.parquet(\"abfs://data@datalakesii.dfs.core.windows.net/DatosOrigen/lr-629/propuesta_f29_comunidades/data_contaminados_with_labels\")\n",
    "contaminacion.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "692a49ed-b6b5-4bdf-829d-483315f2c80b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "join_spark_louvain = louvain.join(contaminacion, louvain[\"RUT_UNICO\"] == contaminacion[\"cont_rut\"], how='left')\n",
    "join_spark_louvain = join_spark_louvain.drop(\"cont_rut\")\n",
    "join_spark_louvain.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e895b1d2-f30f-4f6b-bc6a-30a03cb34aec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "join_spark_louvain.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "967c578f-8cfb-4513-91d8-3de234e60ade",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "oscuridad= spark.read.parquet(\"abfs://data@datalakesii.dfs.core.windows.net/DatoOrigen/lr-629/Oscuridad/final/oscuridad\")\n",
    "oscuridad.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36652eb9-5807-4201-b13b-2eefd04c8d10",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "resultado_intermedio = join_spark_louvain.join(oscuridad, join_spark_louvain[\"RUT_UNICO\"] == oscuridad[\"CONT_RUT\"], how='left')\n",
    "resultado_intermedio  = resultado_intermedio.drop(\"CONT_RUT\")\n",
    "resultado_intermedio.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6c8f614-13e6-4d49-8354-ba65291cff20",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "resultado_intermedio.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f302a500-f6bb-4753-bf64-084b40d73d9c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "familiaridad= spark.read.parquet(\"abfs://data@datalakesii.dfs.core.windows.net/DatoOrigen/lr-629/Oscuridad/final/familiaridad\")\n",
    "familiaridad.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92418b6d-0c0f-4649-ab63-c1e4ebd72c7b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "familiaridad.select(\"RUT_SOCIEDAD\").count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34906b22-b8b7-422b-8ae2-900b3da1fd82",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_spark= resultado_intermedio.join(familiaridad,resultado_intermedio[\"RUT_UNICO\"]==familiaridad[\"RUT_SOCIEDAD\"],how='left')\n",
    "df_spark  = df_spark.drop(\"RUT_SOCIEDAD\")\n",
    "df_spark.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "348f2b27-75c1-4d56-af99-30fd333ac15b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_spark.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb5b45f3-5a27-46a7-9aee-18d0af6a1a36",
   "metadata": {},
   "source": [
    "### Se agrega cantidad de trabajadores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a8c5596-4e37-4aec-ab6d-7cbd3a05fbc9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Paso 1: Cargar el archivo CSV que contiene la información de trabajadores\n",
    "df_trabajadores = spark.read.csv(\"/home/cdsw/data/trabajadores/trabajadores_last_declaration.csv\", header=True, inferSchema=True)\n",
    "\n",
    "# Verificar la estructura del DataFrame de trabajadores\n",
    "df_trabajadores.printSchema()\n",
    "\n",
    "df_trabajadores = df_trabajadores.drop(\"_c0\")\n",
    "# Mostrar una muestra de los datos de trabajadores\n",
    "df_trabajadores.show(5)\n",
    "\n",
    "# Paso 2: Unir df_spark con df_trabajadores utilizando la columna CONT_RUT en ambos DataFrames\n",
    "df_spark_completo = df_spark.join(df_trabajadores, df_spark[\"RUT_UNICO\"]==df_trabajadores[\"CONT_RUT\"], how=\"left\")\n",
    "df_spark_completo = df_spark_completo.drop(\"CONT_RUT\")\n",
    "# Paso 3: Visualizar el DataFrame resultante\n",
    "df_spark_completo.columns\n",
    "\n",
    "df_spark_completo.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73e1847f-98c1-463a-a4fd-9660c0b57e54",
   "metadata": {},
   "source": [
    "### Se agregan los patrimonios"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af00844-c65b-4a2f-bad6-c98c5c9c46b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.read.parquet(\"abfs://data@datalakesii.dfs.core.windows.net/DatoOrigen/lr-629/Agrupacion_empresas_similares/patrimonio/patrimonios_incompletos\").createOrReplaceTempView(\"pat_2022_incompleto\")\n",
    "spark.sql('select  RUT_SOCIEDAD as CONT_RUT, SUM(PAT_2022) as PAT_2022 from pat_2022_incompleto group by RUT_SOCIEDAD').createOrReplaceTempView(\"pat_2022_incompleto\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "941d2d11-78f7-4dc9-8c8a-eb086a960ba0",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Realizar la unión de df_spark_completo con pat_2022_incompleto\n",
    "df_spark_con_pat = df_spark_completo.join(\n",
    "    spark.table(\"pat_2022_incompleto\"),  # Usamos la tabla temporal creada\n",
    "    df_spark_completo[\"RUT_UNICO\"] == spark.table(\"pat_2022_incompleto\")[\"CONT_RUT\"],\n",
    "    how=\"left\"  # Unión por la columna CONT_RUT, y 'left' para mantener todas las filas de df_spark_completo\n",
    ")\n",
    "df_spark_con_pat.drop(\"cont_rut\")\n",
    "df_spark_con_pat.printSchema()\n",
    "df_spark_con_pat.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12b61f37-9f44-4027-8b5c-371b7a675a3a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Seleccionar y renombrar las columnas\n",
    "df_contribuyentes = df_spark_con_pat.select(\n",
    "    \"RUT_UNICO\", \"SOCIEDAD\", \"CONTADOR\",\"NATURAL\",\"REPRESENTANTE\",\n",
    "    df_spark_con_pat[\"score\"].alias(\"Contaminacion\"),\n",
    "    df_spark_con_pat[\"Total_pago_f29\"],\n",
    "    df_spark_con_pat[\"IVA_neto\"],\n",
    "    df_spark_con_pat[\"Unidad_regional\"],\n",
    "    df_spark_con_pat[\"n_documentos\"].alias(\"Numero_documentos\"),\n",
    "    df_spark_con_pat[\"lifetime\"].alias(\"Tiempo_de_vida_dias\"),\n",
    "    df_spark_con_pat[\"Alerta_inicial\"],\n",
    "    df_spark_con_pat[\"Value\"].alias(\"Oscuridad\"),\n",
    "    df_spark_con_pat[\"FAMILIARES\"].alias(\"Familiares\"),\n",
    "    df_spark_con_pat[\"TOTAL\"].alias(\"Personas_naturales_socios\"),\n",
    "    df_spark_con_pat[\"TASA_FAMILIARIDAD\"].alias(\"Tasa_familiaridad\"),\n",
    "    df_spark_con_pat[\"COM_INICIAL\"].alias(\"Grupo_economico_inicial\"),\n",
    "    df_spark_con_pat[\"`comunidad_0.109257`\"].alias(\"Grupo_louvain\"),\n",
    "    df_spark_con_pat[\"cantidad_trabajadores_honorarios\"].alias(\"Trabajadores_honorarios\"),\n",
    "    df_spark_con_pat[\"cantidad_trabajadores_dependientes\"].alias(\"Trabajadores_dependientes\"),\n",
    "    df_spark_con_pat[\"PAT_2022\"].alias(\"Patrimonio_2022\")\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "291c43a3-4425-4af1-9efc-12dcc641c090",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df_contribuyentes.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f131aa7-801a-4b60-8be1-01b517e5b974",
   "metadata": {},
   "source": [
    "### Ventas a regiones extremas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e3748b6-10e1-4fa5-a0a5-c99592f017e1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#INICIALMENTE A INCLUIR PERO DEBIDO A PROBLEMAS DE PROCESAMIENTO NO INCLUIDO EN LA VERSION FINAL\n",
    "\n",
    "# Crear la vista temporal a partir del dataframe df_contribuyentes\n",
    "\"\"\"df_contribuyentes.createOrReplaceTempView(\"contribuyentes\")\"\"\"\n",
    "\n",
    "# Realizar la consulta seleccionando solo los dhdr_rut_emisor que están en RUT_UNICO de df_contribuyentes\n",
    "\"\"\"spark.sql('''\n",
    "SELECT d.dhdr_rut_emisor, d.dhdr_cmna_origen\n",
    "FROM dwbgdata.header_dte_consolidada_enc_sas_analitica d\n",
    "INNER JOIN contribuyentes c\n",
    "ON d.dhdr_rut_emisor = c.RUT_UNICO\n",
    "''').createOrReplaceTempView(\"documentos_filtrados\")\"\"\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aa76994b-9cb3-4e18-9747-e8892d74d00f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "spark.sql('select dhdr_rut_emisor,dhdr_cmna_origen from documentos_filtrados').createOrReplaceTempView(\"emitidos\")\n",
    "spark.sql('select * from libsdf.DICCIONARIO_CMNA_DTE_ARFI').createOrReplaceTempView(\"comunas\")\n",
    "spark.sql('select dhdr_rut_emisor,comuna_obtenida from emitidos left join comunas on emitidos.dhdr_cmna_origen=comunas.dhdr_dir_origen').createOrReplaceTempView(\"puntos_venta\")\n",
    "\n",
    "spark.sql('select dhdr_rut_emisor, comuna_obtenida, count(*) as c  from puntos_venta where dhdr_rut_emisor is not null group by dhdr_rut_emisor,comuna_obtenida').createOrReplaceTempView(\"puntos_venta\")\n",
    "\n",
    "spark.sql('select dhdr_rut_emisor,REPLACE(comuna_obtenida,\"�\", \"N\") AS comuna_obtenida from puntos_venta').createOrReplaceTempView(\"puntos_venta\")\n",
    "\n",
    "spark.sql('select COMU_DES_COMUNA, COMU_DES_REGION from dw.dim_comuna').createOrReplaceTempView(\"comunas\")\n",
    "\n",
    "spark.sql('select  dhdr_rut_emisor,comuna_obtenida,COMU_DES_COMUNA, COMU_DES_REGION from puntos_venta left join comunas on puntos_venta.comuna_obtenida=comunas.COMU_DES_COMUNA Where comuna_obtenida is not null and COMU_DES_COMUNA IS not  NULL').createOrReplaceTempView(\"puntos_venta_final\")\n",
    "\n",
    "df=spark.sql('select * from puntos_venta_final')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77be6cb4-ccfc-4c95-a04d-c98880c4eb80",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Definir los patrones de las regiones extremas\n",
    "# I region de Tarapaca\n",
    "# XV reion de Arica y Parinacota\n",
    "# XI regino de Aysen\n",
    "#XII region de Magallanes\n",
    "\"\"\"\n",
    "extreme_regions_norte = [\"XV R\", \"I RE\"]\n",
    "extreme_regions_sur = [\"XI R\",\"XII \"]\n",
    "\n",
    "# Añadir una nueva columna con los primeros tres caracteres de la región\n",
    "df = df.withColumn(\"region_prefix\", substring(col(\"COMU_DES_REGION\"), 1, 4))\n",
    "\n",
    "# Clasificar las regiones\n",
    "df = df.withColumn(\n",
    "    \"extremo_norte\",\n",
    "    when(col(\"region_prefix\").isin(extreme_regions_norte), 1).otherwise(0)\n",
    ")\n",
    "df = df.withColumn(\n",
    "    \"extremo_sur\",\n",
    "    when(col(\"region_prefix\").isin(extreme_regions_sur), 1).otherwise(0)\n",
    ")\n",
    "\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8955d34c-21bf-4f27-88da-7b7d4943c149",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Agrupar por dhdr_rut_emisor y calcular la suma de extremo_norte y extremo_sur\n",
    "\"\"\"df_grouped = df.groupBy(\"dhdr_rut_emisor\").agg(\n",
    "    sum(\"extremo_norte\").alias(\"total_extremo_norte\"),\n",
    "    sum(\"extremo_sur\").alias(\"total_extremo_sur\")\n",
    ")\n",
    "\n",
    "# Crear una columna adicional que indique si distribuye al extremo norte y al extremo sur\n",
    "df_extremos = df_grouped.withColumn(\n",
    "    \"distribuye_extremos\",\n",
    "    when((col(\"total_extremo_norte\") > 0) & (col(\"total_extremo_sur\") > 0), \"si\").otherwise(\"no\")\n",
    ")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0800309e-dd99-43f1-9701-2b00aadbb0bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"df_extremos.count()\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dfa1c41-e29f-4033-9aa5-5cd81bdf5bdb",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"df_extremos.write.mode('overwrite').format(\"parquet\").save(\"abfs://data@datalakesii.dfs.core.windows.net/DatosOrigen/lr-629/riesgo_fraude/proyecto_visualizacion_racionales_positivos/data_puntos_venta\")\n",
    "df_extremos=spark.read.parquet(\"abfs://data@datalakesii.dfs.core.windows.net/DatosOrigen/lr-629/riesgo_fraude/proyecto_visualizacion_racionales_positivos/data_puntos_venta\")\n",
    "df_extremos.count()\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51eec07d-de1f-428f-aa65-697405fd5b6e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Realizar el left join entre df_contribuyentes y df_extremos en base a las columnas especificadas\n",
    "\"\"\"df_final = df_contribuyentes.join(\n",
    "    df_extremos.select('dhdr_rut_emisor','distribuye_extremos'),  # Seleccionamos las columnas de df_extremos\n",
    "    df_contribuyentes['RUT_UNICO'] == df_extremos['dhdr_rut_emisor'],  # Condición del join\n",
    "    how='left'  # Especificamos que sea un left join\n",
    ")\n",
    "\n",
    "\n",
    "# Eliminar la columna 'dhdr_rut_emisor' del resultado final, ya que no la deseas en el DataFrame final\n",
    "df_final = df_final.drop(df_extremos['dhdr_rut_emisor'])\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d249a1c-77ad-4014-8e99-51ff22a81a96",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"df_final.columns\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17baa074-7555-44e0-81ae-9adb9c7743b1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\"\"\"df_final.count()\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1d613fa-84df-44f9-a399-7bb081dd5849",
   "metadata": {},
   "source": [
    "### Remanente contaminado"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68f4de33-519a-459b-ad7b-64730444c3ff",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "remanente=spark.read.parquet(\"abfs://data@datalakesii.dfs.core.windows.net/DatosOrigen/lr-629/riesgo_fraude/remanente_contaminado/remanente_hist_updated\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c26e30c-4b79-4a9d-b404-2b5460c726c9",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "remanente.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68bef506-1443-4ecd-a187-e5df28823126",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Definir una ventana particionada por \"receptor\" y ordenada por \"dcv_ptributario\" en orden descendente\n",
    "window_spec = Window.partitionBy(\"receptor\").orderBy(F.desc(\"dcv_ptributario\"))\n",
    "\n",
    "# Agregar una columna de rank que identifique el valor máximo por receptor\n",
    "remanente_con_rank = remanente.withColumn(\"rank\", F.rank().over(window_spec))\n",
    "\n",
    "# Filtrar para quedarse solo con el valor máximo de \"dcv_ptributario\" para cada \"receptor\"\n",
    "resultado_max = remanente_con_rank.filter(F.col(\"rank\") == 1)\n",
    "\n",
    "# Seleccionar las columnas deseadas: \"dcv_ptributario\", \"receptor\" y \"remanente_cont\"\n",
    "remanente_final = resultado_max.select(\"dcv_ptributario\", \"receptor\", \"remanente_cont\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f07ed9b5-5f42-491b-9137-cf4fd65cdb00",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Realizar el join entre df_contribuyentes y remanente_final en las columnas RUT_UNICO y receptor\n",
    "joined_df = df_contribuyentes.join(remanente_final, df_contribuyentes[\"RUT_UNICO\"] == remanente_final[\"receptor\"], \"left\")\n",
    "\n",
    "# Renombrar las columnas 'dcv_ptributario' a 'ultimo_periodo_remanente_contaminado'\n",
    "# y mantener 'remanente_cont' sin cambios\n",
    "joined_df_renamed = joined_df.withColumnRenamed(\"dcv_ptributario\", \"ultimo_periodo_remanente_contaminado\")\n",
    "\n",
    "# Eliminar la columna 'receptor' ya que no se quiere en el resultado final\n",
    "final_df = joined_df_renamed.drop(\"receptor\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ee4aadf-a96f-4b10-a4cf-42a5bc0ab337",
   "metadata": {},
   "outputs": [],
   "source": [
    "final_df.createOrReplaceTempView('contribuyentes')\n",
    "final_df.write.mode('overwrite').format(\"parquet\").save(\"abfs://data@datalakesii.dfs.core.windows.net/DatosOrigen/lr-629/riesgo_fraude/proyecto_visualizacion_racionales_positivos/data_contribuyentes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4751cda2-8d7e-4927-af34-9c290edd3270",
   "metadata": {},
   "source": [
    "### Calculo de flujo de IVA intra y extragrupo para cada contribuyente"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99b1e613-dc7b-4907-81fd-41a0f2e01f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.read.parquet(\"abfs://data@datalakesii.dfs.core.windows.net/DatosOrigen/LibSDF/JBA_ARCOS_E\").createOrReplaceTempView(\"arcos\")\n",
    "spark.sql(\"SELECT PARU_RUT_E0, PARU_RUT_E2, Monto_IVA FROM arcos where Monto_IVA>0 order by PARU_RUT_E2 asc\").createOrReplaceTempView(\"arcos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfba32c6-33b0-427a-b201-59b97f97a9ca",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "spark.sql('''\n",
    "    SELECT \n",
    "        arcos.PARU_RUT_E0 AS emisor,\n",
    "        c1.grupo_louvain AS comunidad_emisor,\n",
    "        arcos.PARU_RUT_E2 AS receptor,\n",
    "        c2.grupo_louvain AS comunidad_receptor,\n",
    "        arcos.Monto_IVA\n",
    "    FROM arcos\n",
    "    LEFT JOIN contribuyentes AS c1 ON arcos.PARU_RUT_E0 = c1.RUT_UNICO\n",
    "    LEFT JOIN contribuyentes AS c2 ON arcos.PARU_RUT_E2 = c2.RUT_UNICO\n",
    "    WHERE c1.grupo_louvain IS NOT NULL OR c2.grupo_louvain IS NOT NULL\n",
    "''').createOrReplaceTempView('arcos')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99f8396e-0725-49cf-a09b-a7b6449130d6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "resultados_emisor = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        emisor,\n",
    "        SUM(CASE \n",
    "                WHEN comunidad_emisor = comunidad_receptor THEN Monto_IVA \n",
    "                ELSE 0 \n",
    "            END) AS total_iva_intragrupo,\n",
    "        SUM(CASE \n",
    "                WHEN comunidad_emisor <> comunidad_receptor AND comunidad_receptor IS NOT NULL THEN Monto_IVA \n",
    "                ELSE 0 \n",
    "            END) AS total_iva_extragrupo,\n",
    "        COUNT(CASE \n",
    "                WHEN comunidad_emisor = comunidad_receptor THEN 1 \n",
    "                ELSE NULL \n",
    "            END) AS arcos_intragrupo_emisor,\n",
    "        COUNT(CASE \n",
    "                WHEN comunidad_emisor <> comunidad_receptor AND comunidad_receptor IS NOT NULL THEN 1 \n",
    "                ELSE NULL \n",
    "            END) AS arcos_extragrupo_emisor\n",
    "    FROM \n",
    "        arcos\n",
    "    WHERE \n",
    "        comunidad_emisor IS NOT NULL OR comunidad_receptor IS NOT NULL\n",
    "    GROUP BY \n",
    "        emisor\n",
    "\"\"\")\n",
    "\n",
    "resultados_emisor.createOrReplaceTempView('resultados_emisor')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd38404e-ecee-4946-a43a-56c8371a1cc4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "resultados_receptor = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        receptor,\n",
    "        SUM(CASE \n",
    "                WHEN comunidad_receptor = comunidad_emisor THEN Monto_IVA \n",
    "                ELSE 0 \n",
    "            END) AS total_iva_intragrupo_receptor,\n",
    "        SUM(CASE \n",
    "                WHEN comunidad_receptor <> comunidad_emisor AND comunidad_emisor IS NOT NULL THEN Monto_IVA \n",
    "                ELSE 0 \n",
    "            END) AS total_iva_extragrupo_receptor,\n",
    "        COUNT(CASE \n",
    "                WHEN comunidad_receptor <> comunidad_emisor AND comunidad_emisor IS NOT NULL \n",
    "                THEN 1 \n",
    "                ELSE NULL \n",
    "            END) AS arcos_extragrupo_receptor,\n",
    "        COUNT(CASE \n",
    "                WHEN comunidad_receptor = comunidad_emisor THEN 1 \n",
    "                ELSE NULL \n",
    "            END) AS arcos_intragrupo_receptor\n",
    "    FROM \n",
    "        arcos\n",
    "    WHERE \n",
    "        comunidad_receptor IS NOT NULL OR comunidad_emisor IS NOT NULL\n",
    "    GROUP BY \n",
    "        receptor\n",
    "\"\"\")\n",
    "\n",
    "\n",
    "# Muestra el resultado para el receptor\n",
    "resultados_receptor.createOrReplaceTempView('resultados_receptor')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9a363b8-2b6a-4c6d-a077-3b396f4fb5da",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "resultados_final = spark.sql(\"\"\"\n",
    "    SELECT \n",
    "        COALESCE(e.emisor, r.receptor) AS contribuyente,\n",
    "        COALESCE(e.total_iva_intragrupo, 0) AS total_iva_intragrupo_emisor,\n",
    "        COALESCE(e.total_iva_extragrupo, 0) AS total_iva_extragrupo_emisor,\n",
    "        COALESCE(r.total_iva_intragrupo_receptor, 0) AS total_iva_intragrupo_receptor,\n",
    "        COALESCE(r.total_iva_extragrupo_receptor, 0) AS total_iva_extragrupo_receptor,\n",
    "        arcos_intragrupo_emisor,\n",
    "        arcos_extragrupo_emisor,\n",
    "        arcos_intragrupo_receptor,\n",
    "        arcos_extragrupo_receptor\n",
    "        \n",
    "    FROM \n",
    "        (SELECT emisor, total_iva_intragrupo, total_iva_extragrupo,arcos_intragrupo_emisor,arcos_extragrupo_emisor FROM resultados_emisor) e\n",
    "    FULL OUTER JOIN \n",
    "        (SELECT receptor, total_iva_intragrupo_receptor, total_iva_extragrupo_receptor,arcos_intragrupo_receptor,arcos_extragrupo_receptor FROM resultados_receptor) r\n",
    "    ON e.emisor = r.receptor\n",
    "\"\"\")\n",
    "\n",
    "# Muestra el resultado final\n",
    "resultados_final.limit(10).show()\n",
    "resultados_final.write.mode('overwrite').format(\"parquet\").save(\"abfs://data@datalakesii.dfs.core.windows.net/DatosOrigen/lr-629/riesgo_fraude/proyecto_visualizacion_racionales_positivos/data_iva_contribuyentes\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "127ecafc-44ef-440d-a1bd-c93228dbd8ba",
   "metadata": {},
   "source": [
    "### Calculo de flujo de IVA intra y extragrupo total para comunidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e17e3b83-d64c-4daf-b0fc-e62c8bcdaa54",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# Emisión intra-grupo\n",
    "spark.sql(\"\"\"\n",
    "    SELECT comunidad_emisor AS comunidad, SUM(Monto_IVA) AS emision_intragrupo, COUNT(*) AS arcos_emision_intra\n",
    "    FROM arcos\n",
    "    WHERE comunidad_emisor = comunidad_receptor AND comunidad_emisor IS NOT NULL\n",
    "    GROUP BY comunidad_emisor\n",
    "    ORDER BY comunidad_emisor ASC\n",
    "\"\"\").createOrReplaceTempView('emision_intra')\n",
    "\n",
    "# Emisión extra-grupo\n",
    "spark.sql(\"\"\"\n",
    "    SELECT comunidad_emisor AS comunidad, SUM(Monto_IVA) AS emision_extragrupo, COUNT(*) AS arcos_emision_extra\n",
    "    FROM arcos\n",
    "    WHERE comunidad_emisor <> comunidad_receptor AND comunidad_emisor IS NOT NULL\n",
    "    GROUP BY comunidad_emisor\n",
    "    ORDER BY comunidad_emisor ASC\n",
    "\"\"\").createOrReplaceTempView('emision_extra')\n",
    "\n",
    "# Recepción intra-grupo\n",
    "spark.sql(\"\"\"\n",
    "    SELECT comunidad_receptor AS comunidad, SUM(Monto_IVA) AS recepcion_intragrupo, COUNT(*) AS arcos_reception_intra\n",
    "    FROM arcos\n",
    "    WHERE comunidad_emisor = comunidad_receptor AND comunidad_receptor IS NOT NULL\n",
    "    GROUP BY comunidad_receptor\n",
    "    ORDER BY comunidad_receptor ASC\n",
    "\"\"\").createOrReplaceTempView('recepcion_intra')\n",
    "\n",
    "# Recepción extra-grupo\n",
    "spark.sql(\"\"\"\n",
    "    SELECT comunidad_receptor AS comunidad, SUM(Monto_IVA) AS recepcion_extragrupo, COUNT(*) AS arcos_reception_extra\n",
    "    FROM arcos\n",
    "    WHERE comunidad_emisor <> comunidad_receptor AND comunidad_receptor IS NOT NULL\n",
    "    GROUP BY comunidad_receptor\n",
    "    ORDER BY comunidad_receptor ASC\n",
    "\"\"\").createOrReplaceTempView('recepcion_extra')\n",
    "\n",
    "# Unir emisión intra y extra en una sola tabla\n",
    "spark.sql(\"\"\"\n",
    "    SELECT COALESCE(emision_intra.comunidad, emision_extra.comunidad) AS com, \n",
    "           emision_intragrupo, emision_extragrupo, arcos_emision_intra, arcos_emision_extra\n",
    "    FROM emision_intra\n",
    "    FULL OUTER JOIN emision_extra ON emision_extra.comunidad = emision_intra.comunidad\n",
    "\"\"\").createOrReplaceTempView('emision')\n",
    "\n",
    "# Unir recepción intra y extra en una sola tabla\n",
    "spark.sql(\"\"\"\n",
    "    SELECT COALESCE(recepcion_intra.comunidad, recepcion_extra.comunidad) AS com, \n",
    "           recepcion_intragrupo, recepcion_extragrupo, arcos_reception_intra as arcos_recepcion_intra, arcos_reception_extra as arcos_recepcion_extra\n",
    "    FROM recepcion_intra\n",
    "    FULL OUTER JOIN recepcion_extra ON recepcion_extra.comunidad = recepcion_intra.comunidad\n",
    "    ORDER BY recepcion_intra.comunidad ASC\n",
    "\"\"\").createOrReplaceTempView('recepcion')\n",
    "\n",
    "# Unir las tablas de emisión y recepción\n",
    "spark.sql(\"\"\"\n",
    "    SELECT COALESCE(emision.com, recepcion.com) AS com, \n",
    "           emision_intragrupo, emision_extragrupo, \n",
    "           recepcion_intragrupo, recepcion_extragrupo,\n",
    "           arcos_emision_intra, arcos_emision_extra, \n",
    "           arcos_recepcion_intra, arcos_recepcion_extra\n",
    "    FROM emision\n",
    "    FULL OUTER JOIN recepcion ON emision.com = recepcion.com\n",
    "\"\"\").createOrReplaceTempView('iva_comunidad')\n",
    "\n",
    "# Cálculos adicionales\n",
    "iva_comunidad=spark.sql(\"\"\"\n",
    "    SELECT com, \n",
    "           (emision_extragrupo / (emision_intragrupo + emision_extragrupo)) * 100 AS perct_emision_extra,\n",
    "           (recepcion_extragrupo / (recepcion_intragrupo + recepcion_extragrupo)) * 100 AS perct_recepcion_extra,\n",
    "           emision_extragrupo, recepcion_extragrupo,\n",
    "           emision_intragrupo, recepcion_intragrupo,\n",
    "           emision_extragrupo / emision_intragrupo AS tasa_emision_extra_intra,\n",
    "           arcos_emision_intra, arcos_emision_extra,\n",
    "           arcos_recepcion_intra, arcos_recepcion_extra\n",
    "    FROM iva_comunidad\n",
    "\"\"\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "030b7e59-348a-462f-90b6-6deb97e8d8c2",
   "metadata": {},
   "source": [
    "### Se agrega tamanio de cada comunidad"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b59ad2d-4487-49a0-b8f9-31585211e7ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paso 1: Contar el tamaño de cada comunidad en la tabla `contribuyentes`\n",
    "grupo_tamanio = df_contribuyentes.groupBy(\"grupo_louvain\").count().withColumnRenamed(\"count\", \"tamanio_comunidad\")\n",
    "\n",
    "# Paso 2: Unir la información del tamaño de comunidad con la tabla IVA_comunidad\n",
    "iva_comunidad_con_tamanio = iva_comunidad.join(grupo_tamanio, iva_comunidad[\"com\"] == grupo_tamanio[\"grupo_louvain\"], how=\"left\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed68b578-a531-4d03-852b-fb975ed9df06",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "iva_comunidad_con_tamanio.write.mode('overwrite').format(\"parquet\").save(\"abfs://data@datalakesii.dfs.core.windows.net/DatosOrigen/lr-629/riesgo_fraude/proyecto_visualizacion_racionales_positivos/data_iva_dashboard\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d44ec17-7a8e-4ca4-b2f0-bfbcb5af0b40",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dda0d570-0aa7-4ec0-9cfe-e80ae47f9bfe",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
